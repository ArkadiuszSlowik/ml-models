{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "# from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "# from platform import python_version\n",
    "# print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normal Equation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_NE(X:np.ndarray, y:np.ndarray, X_test:np.ndarray, y_pred:np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Normal Equation.\n",
    "    by Aurélien Géron\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    \n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    y_pred = X_test_b.dot(theta_best)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Singular Value Decomposition </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_SVD(X:np.ndarray, y:np.ndarray, X_test:np.ndarray, y_pred:np.ndarray, threshold=0.0001) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Simplified implementation of linear regression using Singular Value Decomposition.\n",
    "    \n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"\n",
    "    \n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    # Calculate SVD\n",
    "    U, E_vec, V_t = np.linalg.svd(X_b)\n",
    "\n",
    "    # Calculate pseudoinverse\n",
    "\n",
    "    for i in range(len(E_vec)):\n",
    "        if E_vec[i] < threshold:\n",
    "            E_vec[i] = 0\n",
    "        else:\n",
    "            E_vec[i] = 1 / E_vec[i]\n",
    "\n",
    "    E_vec[E_vec < threshold] = 0\n",
    "    E_ = np.vstack([np.diag(E_vec), np.zeros([X_b.shape[0] - len(np.diag(E_vec)), X_b.shape[1]])])\n",
    "    X_b_ = V_t.T.dot(E_.T).dot(U.T)\n",
    "\n",
    "    # Calculate theta\n",
    "    theta = X_b_.dot(y)\n",
    "\n",
    "    # Prediction\n",
    "\n",
    "    y_pred = X_test_b.dot(theta)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_BGD(X:np.ndarray, y:np.ndarray, X_test:np.ndarray, y_pred:np.ndarray, theta:list, tolerance:float=0.0001, message_frequency:int=20, \n",
    "n_iterations:int=500, eta:float=0.01, debugger:bool=True) -> Tuple[float, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Batch Gradient Descent.\n",
    "    mostly by Aurélien Géron\n",
    "    - slow for large m\n",
    "    - no out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 hyperparameters\n",
    "    - scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    m = X_b.shape[0]\n",
    "    theta_path_BGD = [theta]\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        gradient = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradient\n",
    "        theta_path_BGD.append(theta)\n",
    "        \n",
    "        if np.linalg.norm(theta_path_BGD[-1]-theta_path_BGD[-2]) < tolerance:\n",
    "            break\n",
    "        if i % message_frequency == 0:\n",
    "            print('theta: ',theta)\n",
    "        if debugger and not np.isfinite(theta).all():\n",
    "            print('eta too large!')\n",
    "            break\n",
    "            \n",
    "    theta_best = theta\n",
    "    theta_path_BGD = np.array(theta_path_BGD)\n",
    "    \n",
    "    y_pred = X_test_b.dot(theta)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return theta, theta_path_BGD, y_pred, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_SGD(X:np.ndarray, y:np.ndarray, X_test:np.ndarray, y_pred:np.ndarray, theta:list,\n",
    "                          n_epochs:int=10, t0:float=10, t1:float=100, message_frequency:int=20,\n",
    "                          tolerance:float=0.0001, debugger:bool=True) -> Tuple[float, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "    mostly by Aurélien Géron\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def learning_rate(t):\n",
    "        return t0 / (t + t1)\n",
    "\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    theta_path_SGD = [theta]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(m):\n",
    "            random_sample = np.random.randint(0, m)\n",
    "            x_i = X_b[random_sample]\n",
    "            y_i = y[random_sample]\n",
    "            gradient = 2 * x_i.T.dot(x_i.dot(theta) - y_i)\n",
    "            eta = learning_rate(i * m +j)\n",
    "            theta = theta - eta * gradient\n",
    "            theta_path_SGD.append(theta)\n",
    "            \n",
    "            if np.linalg.norm(theta_path_SGD[-1]-theta_path_SGD[-2]) < tolerance:\n",
    "                break\n",
    "            if j % message_frequency == 0:\n",
    "                print('theta: ',theta)\n",
    "            if debugger and not np.isfinite(theta).all():\n",
    "                print('eta too large!')\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    theta_best = theta\n",
    "    theta_path_SGD = np.array(theta_path_SGD)\n",
    "    \n",
    "    y_pred = X_test_b.dot(theta_best)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return theta, theta_path_SGD, y_pred, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_MbGD(X:np.ndarray, y:np.ndarray, X_test:np.ndarray, y_pred:np.ndarray, theta:list,\n",
    "                          batch_size:int, n_epochs:int=10, t0:float=10, t1:float=100, message_frequency:int=20,\n",
    "                          tolerance:float=0.0001, debugger:bool=True) -> Tuple[float, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Mini-batch Gradient Descent.\n",
    "    \n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def learning_rate(t):\n",
    "        return t0 / (t + t1)\n",
    "    \n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    theta_path_MbGD = [theta]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(m):\n",
    "            random_samples = random.sample(range(0, m), batch_size)\n",
    "            x_i = X_b[random_samples]\n",
    "            y_i = y[random_samples]\n",
    "            gradients = 2 / batch_size * x_i.T.dot(x_i.dot(theta) - y_i)\n",
    "            eta = learning_rate(i * m +j)\n",
    "            theta = theta - eta * gradients\n",
    "            theta_path_MbGD.append(theta)\n",
    "            \n",
    "            if np.linalg.norm(theta_path_MbGD[-1]-theta_path_MbGD[-2]) < tolerance:\n",
    "                break\n",
    "            if j % message_frequency == 0:\n",
    "                print('theta: ',theta)\n",
    "            if debugger and not np.isfinite(theta).all():\n",
    "                print('eta too large!')\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "        \n",
    "    theta_best = theta\n",
    "    theta_path_MbGD = np.array(theta_path_MbGD)\n",
    "    \n",
    "    y_pred = X_test_b.dot(theta_best)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return theta, theta_path_MbGD, y_pred, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_compare_paths(paths:list, path_labels:list, figsize:tuple=(7,4), legend_loc:str='upper left', legend_fontsize:int=16,\n",
    "                                   label_fontize:int=20, line_styles:list=['r-s', 'g-+', 'b-o']):\n",
    "    \"\"\"\n",
    "    Plots theta paths for different linear regression step by step implementations.\n",
    "    mostly by Aurélien Géron\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(len(paths)):\n",
    "        line_style = line_styles[i%len(line_styles)]\n",
    "        path_label = 'path ' + str(i)\n",
    "        plt.plot(paths[i][:, 0], paths[i][:, 1], line_style, linewidth=1+i, label=path_labels[i])\n",
    "        \n",
    "\n",
    "    #plt.plot(theta_path_BGD[:, 0], theta_path_BGD[:, 1], \"r-s\", linewidth=1, label=\"Batch\")\n",
    "\n",
    "    plt.legend(loc=legend_loc, fontsize=legend_fontsize)\n",
    "    plt.xlabel(r\"$\\theta_0$\", fontsize=label_fontize)\n",
    "    plt.ylabel(r\"$\\theta_1$\", fontsize=label_fontize, rotation=0)\n",
    "    plt.axis()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

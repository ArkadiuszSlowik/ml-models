{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "# from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('always', Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear Regression </h2>\n",
    "\n",
    "Regularization:\n",
    "\n",
    "- It is almost always preferable to have at least a little bit of regularization -> generally avoid plain Linear Regression.\n",
    "\n",
    "- Ridge is a good default, but\n",
    "\n",
    "- if only a few features are useful, you should prefer Lasso or Elastic Net -> they tend to reduce the useless features’ weights down to zero.\n",
    "\n",
    "- Elastic Net is preferred over Lasso -> Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "~ Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normal Equation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_NE:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Normal Equation.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b)).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Singular Value Decomposition </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SVD:\n",
    "    \"\"\"\n",
    "    Simplified implementation of linear regression using Singular Value Decomposition.\n",
    "    \n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.U = None\n",
    "        self.E_vec = None\n",
    "        self.V_t = None\n",
    "        self.E_ = None\n",
    "        self.X_b_ = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, threshold:float=0.0001):\n",
    "        \n",
    "    \n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Calculate SVD\n",
    "        self.U, self.E_vec, self.V_t = np.linalg.svd(self.X_b)\n",
    "\n",
    "        # Calculate pseudoinverse\n",
    "\n",
    "        for i in range(len(self.E_vec)):\n",
    "            if self.E_vec[i] < threshold:\n",
    "                self.E_vec[i] = 0\n",
    "            else:\n",
    "                self.E_vec[i] = 1 / self.E_vec[i]\n",
    "\n",
    "        self.E_vec[self.E_vec < threshold] = 0\n",
    "        self.E_ = np.vstack([np.diag(self.E_vec), np.zeros([self.X_b.shape[0] - len(np.diag(self.E_vec)), self.X_b.shape[1]])])\n",
    "        self.X_b_ = self.V_t.T.dot(self.E_.T).dot(self.U.T)\n",
    "\n",
    "        # Calculate theta\n",
    "        self.theta = self.X_b_.dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Batch Gradient Descent.\n",
    "\n",
    "    - slow for large m1. Linear Regression vs Ridge Regression vs Lasso Regression vs Elastic Net Regression.\n",
    "\n",
    "    - no out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 hyperparameters\n",
    "    - scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path_BGD = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, theta:np.ndarray=np.array([0,0]), tolerance:float=0.0001,\n",
    "            n_iterations:int=500, eta:float=0.01, eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        self.theta = theta\n",
    "        self.theta_path_BGD = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            if regularization == None:\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path_BGD.append(self.theta)\n",
    "            \n",
    "            if np.linalg.norm(self.theta_path_BGD[-1]-self.theta_path_BGD[-2]) < tolerance:\n",
    "                break\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path_BGD = np.array(self.theta_path_BGD)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    def theta_info(self):\n",
    "        return self.theta, self.theta_path_BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path_SGD = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, theta:np.ndarray=np.array([0,0]),\n",
    "                          n_epochs:int=10, t0:float=10, t1:float=100,\n",
    "                          tolerance:float=0.0001, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "        self.theta = theta\n",
    "        self.theta_path_SGD = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = y[self.random_sample]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path_SGD.append(self.theta)\n",
    "                \n",
    "                if np.linalg.norm(self.theta_path_SGD[-1]-self.theta_path_SGD[-2]) < tolerance:\n",
    "                    break\n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path_SGD = np.array(self.theta_path_SGD)\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    def theta_info(self):\n",
    "        return self.theta, self.theta_path_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path_MbGD = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, batch_size_ratio:float, theta:np.ndarray=np.array([0,0]),\n",
    "                          n_epochs:int=10, t0:float=10, t1:float=100,\n",
    "                          tolerance:float=0.0001, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "        self.theta = theta\n",
    "        self.theta_path_MbGD = [theta]\n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path_MbGD.append(self.theta)\n",
    "                \n",
    "                if np.linalg.norm(self.theta_path_MbGD[-1]-self.theta_path_MbGD[-2]) < tolerance:\n",
    "                    break\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path_MbGD = np.array(self.theta_path_MbGD)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    def theta_info(self):\n",
    "        return self.theta, self.theta_path_MbGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ridge Regression - Closed-form </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ridge_regression_Cf:\n",
    "    \"\"\"\n",
    "    Implementation of ridge regression using Closed-form.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        self.A = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, alpha:float=0.1):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.A = np.identity(self.X_b.shape[1])\n",
    "        self.A[0][0] = 0\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b) + alpha * self.A).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def mse(self, y_test, y_pred) -> float:\n",
    "        return mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tools </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_compare_paths(paths:list, path_labels:list, figsize:tuple=(7,4), legend_loc:str='upper left', legend_fontsize:int=16,\n",
    "                                   label_fontize:int=20, markers:list=['s', '+', 'o']):\n",
    "    \"\"\"\n",
    "    Plots theta (two-dimensional) paths for different linear regression implementations.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(len(paths)):\n",
    "        marker = markers[i%len(markers)]\n",
    "        path_label = 'path ' + str(i)\n",
    "        plt.plot(paths[i][:, 0], paths[i][:, 1], marker=marker, linewidth=1, alpha=0.5, label=path_labels[i])\n",
    "\n",
    "    plt.legend(loc=legend_loc, fontsize=legend_fontsize)\n",
    "    plt.xlabel(r\"$\\theta_x$\", fontsize=label_fontize)\n",
    "    plt.ylabel(r\"$\\theta_y$\", fontsize=label_fontize, rotation=0)\n",
    "    plt.axis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, end_iteration:int, start_iteration:int=1, return_errors=False, model_hyperparameters={}):\n",
    "    \"\"\"\n",
    "    Plots learning curves- functions where y: performance on training set and validation set, x: training set size \n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(start_iteration, end_iteration):\n",
    "        model.fit(X_train[:m], y_train[:m], **model_hyperparameters)\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.axis()\n",
    "    plt.show()\n",
    "    \n",
    "    if return_errors:\n",
    "        return train_errors, val_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

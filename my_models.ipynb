{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from typing import Tuple\n",
    "# from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('always', Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear Regression </h1>\n",
    "\n",
    "Regularization:\n",
    "\n",
    "- It is almost always preferable to have at least a little bit of regularization -> generally avoid plain Linear Regression.\n",
    "\n",
    "- Ridge is a good default, but\n",
    "\n",
    "- if only a few features are useful, you should prefer Lasso or Elastic Net -> they tend to reduce the useless features’ weights down to zero.\n",
    "\n",
    "- Elastic Net is preferred over Lasso -> Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "~ Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normal Equation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_NE:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Normal Equation.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b)).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Singular Value Decomposition </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SVD:\n",
    "    \"\"\"\n",
    "    Simplified implementation of linear regression using Singular Value Decomposition.\n",
    "    \n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.U = None\n",
    "        self.E_vec = None\n",
    "        self.V_t = None\n",
    "        self.E_ = None\n",
    "        self.X_b_ = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, threshold:float=0.0001):\n",
    "        \n",
    "    \n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Calculate SVD\n",
    "        self.U, self.E_vec, self.V_t = np.linalg.svd(self.X_b)\n",
    "\n",
    "        # Calculate pseudoinverse\n",
    "\n",
    "        for i in range(len(self.E_vec)):\n",
    "            if self.E_vec[i] < threshold:\n",
    "                self.E_vec[i] = 0\n",
    "            else:\n",
    "                self.E_vec[i] = 1 / self.E_vec[i]\n",
    "\n",
    "        self.E_vec[self.E_vec < threshold] = 0\n",
    "        self.E_ = np.vstack([np.diag(self.E_vec), np.zeros([self.X_b.shape[0] - len(np.diag(self.E_vec)), self.X_b.shape[1]])])\n",
    "        self.X_b_ = self.V_t.T.dot(self.E_.T).dot(self.U.T)\n",
    "\n",
    "        # Calculate theta\n",
    "        self.theta = self.X_b_.dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Batch Gradient Descent.\n",
    "\n",
    "    - slow for large m1. Linear Regression vs Ridge Regression vs Lasso Regression vs Elastic Net Regression.\n",
    "\n",
    "    - no out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 hyperparameters\n",
    "    - scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_iterations:int, theta:np.ndarray=np.array([0,0]), eta:float=0.01,\n",
    "            eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            if regularization == None:\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path.append(self.theta)\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray,  n_epochs:int, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1,\n",
    "            mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = y[self.random_sample]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_epochs:int, batch_size_ratio:float=0.1, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "        \n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ridge Regression - Closed-form </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ridge_regression_Cf:\n",
    "    \"\"\"\n",
    "    Implementation of ridge regression using Closed-form.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        self.A = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, alpha:float=0.1):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.A = np.identity(self.X_b.shape[1])\n",
    "        self.A[0][0] = 0\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b) + alpha * self.A).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tools </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_compare_paths(paths:list, path_labels:list, figsize:tuple=(7,4), legend_loc:str='upper left', legend_fontsize:int=16,\n",
    "                                   label_fontize:int=20, markers:list=['s', '+', 'o']):\n",
    "    \"\"\"\n",
    "    Plots theta (two-dimensional) paths for different linear regression implementations.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(len(paths)):\n",
    "        marker = markers[i%len(markers)]\n",
    "        path_label = 'path ' + str(i)\n",
    "        plt.plot(paths[i][:, 0], paths[i][:, 1], marker=marker, linewidth=1, alpha=0.5, label=path_labels[i])\n",
    "\n",
    "    plt.legend(loc=legend_loc, fontsize=legend_fontsize)\n",
    "    plt.xlabel(r\"$\\theta_x$\", fontsize=label_fontize)\n",
    "    plt.ylabel(r\"$\\theta_y$\", fontsize=label_fontize, rotation=0)\n",
    "    plt.axis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, end_iteration:int, start_iteration:int=1, return_errors=False, model_hyperparameters={}):\n",
    "    \"\"\"\n",
    "    Plots learning curves- functions where y: performance on training set and validation set, x: training set size \n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(start_iteration, end_iteration):\n",
    "        model.fit(X_train[:m], y_train[:m], **model_hyperparameters)\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.axis()\n",
    "    plt.show()\n",
    "    \n",
    "    if return_errors:\n",
    "        return train_errors, val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(X_train, y_train, X_val, y_val, model, epochs_number):\n",
    "    \"\"\"\n",
    "    Early stopping implementation for iterative learning Linear Regression.\n",
    "    \"\"\"\n",
    "    minimum_val_error = float('inf')\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "    val_errors = []\n",
    "    best_val_errors = []\n",
    "    best_epochs = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs_number):\n",
    "        model.fit(X_train, y_train, 1, theta=model.theta)\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        val_error = np.sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "        val_errors.append(val_error)\n",
    "    \n",
    "        if val_error < minimum_val_error:\n",
    "            minimum_val_error = val_error\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_errors.append(val_error)\n",
    "            best_epochs.append(best_epoch)\n",
    "            \n",
    "    plt.plot(best_epochs, best_val_errors, \"g .\", linewidth=3, label='Best RMSE by epochs')\n",
    "    plt.plot(np.arange(best_epoch+1, epochs_number), val_errors[best_epoch+1:], \"r .\", linewidth=1, label='Worser/equal RMSE since last best')\n",
    "    plt.legend()\n",
    "    plt.xlim((0, epochs_number-1))\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('best_epoch: ', best_epoch)\n",
    "    print('best_theta: ', best_model.theta)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneDimension_decision_boundary(X, y, x_new, y_proba):\n",
    "    \"\"\"\n",
    "    Plots decision boundary for one feature dataset.\n",
    "    \"\"\"\n",
    "    decision_boundary = np.min(x_new[y_proba[:, 1] >= 0.5])\n",
    "    print(np.min(x_new[y_proba[:, 1] >= 0.5]))\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "    plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "    plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "    plt.plot(x_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Class 1\")\n",
    "    plt.plot(x_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Class 0\")\n",
    "    plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "    plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "    plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "    plt.xlabel(\"Feature value\", fontsize=14)\n",
    "    plt.ylabel(\"Probability\", fontsize=14)\n",
    "    plt.legend(loc=\"center left\", fontsize=14)\n",
    "    plt.ylim([-0.02, 1.02])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gradient Descent algorithm also for logistic regression. Ridge, Lasso and Elastic Net regularizations also works here, like for other linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(t):\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_iterations:int, theta:np.ndarray=np.array([0,0]), eta:float=0.01,\n",
    "            eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            if regularization == None:\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + alpha * np.array([0, *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path.append(self.theta)\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([self.y_pred_proba, 1 - self.y_pred_proba]).T\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Stochastic Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray,  n_epochs:int, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1,\n",
    "            mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = y[self.random_sample]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([self.y_pred_proba, 1 - self.y_pred_proba]).T\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Stochastic Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_epochs:int, batch_size_ratio:float=0.1, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "        \n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([self.y_pred_proba, 1 - self.y_pred_proba]).T\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

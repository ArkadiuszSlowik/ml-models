{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from typing import Tuple\n",
    "# from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('always', Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear Regression </h1>\n",
    "\n",
    "Regularization:\n",
    "\n",
    "- It is almost always preferable to have at least a little bit of regularization -> generally avoid plain Linear Regression.\n",
    "\n",
    "- Ridge is a good default, but\n",
    "\n",
    "- if only a few features are useful, you should prefer Lasso or Elastic Net -> they tend to reduce the useless features’ weights down to zero.\n",
    "\n",
    "- Elastic Net is preferred over Lasso -> Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "~ Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normal Equation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_NE:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Normal Equation.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b)).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Singular Value Decomposition </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SVD:\n",
    "    \"\"\"\n",
    "    Simplified implementation of linear regression using Singular Value Decomposition.\n",
    "    \n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.U = None\n",
    "        self.E_vec = None\n",
    "        self.V_t = None\n",
    "        self.E_ = None\n",
    "        self.X_b_ = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, threshold:float=0.0001):\n",
    "        \n",
    "    \n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Calculate SVD\n",
    "        self.U, self.E_vec, self.V_t = np.linalg.svd(self.X_b)\n",
    "\n",
    "        # Calculate pseudoinverse\n",
    "\n",
    "        for i in range(len(self.E_vec)):\n",
    "            if self.E_vec[i] < threshold:\n",
    "                self.E_vec[i] = 0\n",
    "            else:\n",
    "                self.E_vec[i] = 1 / self.E_vec[i]\n",
    "\n",
    "        self.E_vec[self.E_vec < threshold] = 0\n",
    "        self.E_ = np.vstack([np.diag(self.E_vec), np.zeros([self.X_b.shape[0] - len(np.diag(self.E_vec)), self.X_b.shape[1]])])\n",
    "        self.X_b_ = self.V_t.T.dot(self.E_.T).dot(self.U.T)\n",
    "\n",
    "        # Calculate theta\n",
    "        self.theta = self.X_b_.dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Batch Gradient Descent.\n",
    "\n",
    "    - slow for large m1. Linear Regression vs Ridge Regression vs Lasso Regression vs Elastic Net Regression.\n",
    "\n",
    "    - no out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 hyperparameters\n",
    "    - scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_iterations:int, theta:np.ndarray=np.array([0,0]), eta:float=0.01,\n",
    "            eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            if regularization == None:\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = 2/self.m * self.X_b.T.dot(self.X_b.dot(self.theta) - y) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path.append(self.theta)\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Stochastic Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray,  n_epochs:int, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1,\n",
    "            mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = y[self.random_sample]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of linear regression using Mini-batch Gradient Descent.\n",
    "\n",
    "    - fast for large m\n",
    "    - out-of-core support\n",
    "    - fast for large n\n",
    "    - 2 or more hyperparameters\n",
    "    - scaling required\n",
    "    - from sklearn.linear_model import SGDRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_epochs:int, batch_size_ratio:float=0.1, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "        \n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(self.x_i.dot(self.theta) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ridge Regression - Closed-form </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ridge_regression_Cf:\n",
    "    \"\"\"\n",
    "    Implementation of ridge regression using Closed-form.\n",
    "\n",
    "    m - number of training instances, n - number of features\n",
    "\n",
    "    - fast for large m\n",
    "    - no out-of-core support\n",
    "    - slow for large n\n",
    "    - 0 hyperparameters\n",
    "    - no scaling required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_best = None\n",
    "        self.X_test_b = None\n",
    "        self.y_pred = None\n",
    "        self.X_b = None\n",
    "        self.A = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, alpha:float=0.1):\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.A = np.identity(self.X_b.shape[1])\n",
    "        self.A[0][0] = 0\n",
    "        self.theta_best = np.linalg.inv(self.X_b.T.dot(self.X_b) + alpha * self.A).dot(self.X_b.T).dot(y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred = self.X_test_b.dot(self.theta_best)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tools </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_compare_paths(paths:list, path_labels:list, figsize:tuple=(7,4), legend_loc:str='upper left', legend_fontsize:int=16,\n",
    "                                   label_fontize:int=20, markers:list=['s', '+', 'o']):\n",
    "    \"\"\"\n",
    "    Plots theta (two-dimensional) paths for different linear regression implementations.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(len(paths)):\n",
    "        marker = markers[i%len(markers)]\n",
    "        path_label = 'path ' + str(i)\n",
    "        plt.plot(paths[i][:, 0], paths[i][:, 1], marker=marker, linewidth=1, alpha=0.5, label=path_labels[i])\n",
    "\n",
    "    plt.legend(loc=legend_loc, fontsize=legend_fontsize)\n",
    "    plt.xlabel(r\"$\\theta_x$\", fontsize=label_fontize)\n",
    "    plt.ylabel(r\"$\\theta_y$\", fontsize=label_fontize, rotation=0)\n",
    "    plt.axis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, end_iteration:int, start_iteration:int=1, return_errors=False, model_hyperparameters={}):\n",
    "    \"\"\"\n",
    "    Plots learning curves- functions where y: performance on training set and validation set, x: training set size \n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(start_iteration, end_iteration):\n",
    "        model.fit(X_train[:m], y_train[:m], **model_hyperparameters)\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.axis()\n",
    "    plt.show()\n",
    "    \n",
    "    if return_errors:\n",
    "        return train_errors, val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(X_train, y_train, X_val, y_val, model, epochs_number):\n",
    "    \"\"\"\n",
    "    Early stopping implementation for iterative learning Linear Regression.\n",
    "    \"\"\"\n",
    "    minimum_val_error = float('inf')\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "    val_errors = []\n",
    "    best_val_errors = []\n",
    "    best_epochs = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs_number):\n",
    "        model.fit(X_train, y_train, 1, theta=model.theta)\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        val_error = np.sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "        val_errors.append(val_error)\n",
    "    \n",
    "        if val_error < minimum_val_error:\n",
    "            minimum_val_error = val_error\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_errors.append(val_error)\n",
    "            best_epochs.append(best_epoch)\n",
    "            \n",
    "    plt.plot(best_epochs, best_val_errors, \"g .\", linewidth=3, label='Best RMSE by epochs')\n",
    "    plt.plot(np.arange(best_epoch+1, epochs_number), val_errors[best_epoch+1:], \"r .\", linewidth=1, label='Worser/equal RMSE since last best')\n",
    "    plt.legend()\n",
    "    plt.xlim((0, epochs_number-1))\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('best_epoch: ', best_epoch)\n",
    "    print('best_theta: ', best_model.theta)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(X, y, fitted_model):\n",
    "    \"\"\"\n",
    "    Plots decision boundary for 1,2 or 3 feature dataset.\n",
    "    \"\"\"\n",
    "    if X.shape[1] == 1:\n",
    "        \n",
    "        x_new = np.linspace(np.min(X), np.max(X), 1000).reshape(-1, 1)\n",
    "        y_proba = fitted_model.predict_proba(x_new)\n",
    "        decision_boundary = x_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "        plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "        plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "\n",
    "        plt.plot(x_new, y_proba[:, 1], \"g--\", linewidth=2, label=\"Class 1\")\n",
    "        plt.plot(x_new, y_proba[:, 0], \"b-\", linewidth=2, label=\"Class 0\")\n",
    "\n",
    "        plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "        plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "        plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "        plt.xlabel(\"Feature value\", fontsize=14)\n",
    "        plt.ylabel(\"Probability\", fontsize=14)\n",
    "        plt.legend(loc=\"center left\", fontsize=14)\n",
    "        plt.ylim([-0.02, 1.02])\n",
    "        plt.show()\n",
    "    elif X.shape[1] == 2:\n",
    "\n",
    "        theta = fitted_model.theta\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        left_right = np.array([np.min(X[:,0]), np.max(X[:,0])])\n",
    "        down_up = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "\n",
    "        x0, x1 = np.meshgrid(\n",
    "                np.linspace(left_right[0], left_right[1], 500).reshape(-1, 1),\n",
    "                np.linspace(down_up[0], down_up[1], 500).reshape(-1, 1),\n",
    "            )\n",
    "        X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "        y_proba = fitted_model.predict_proba(X_new)\n",
    "\n",
    "        zz = y_proba[:, 1].reshape(x0.shape)\n",
    "        contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "        plt.clabel(contour, inline=1, fontsize=12)\n",
    "\n",
    "        plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "        plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "        boundary = -(theta[1] * left_right + theta[0]) / theta[2] # x_2  = -c / b - a / b * x_1 https://www.youtube.com/watch?v=3qzWeokRYTA&ab_channel=AppliedAICourse\n",
    "\n",
    "        plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "\n",
    "        plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "        plt.ylabel(\"Feature 2\", fontsize=14)\n",
    "\n",
    "        plt.show()\n",
    "            \n",
    "    elif X.shape[1] == 3:\n",
    "            \n",
    "       # Interactions with plot may not work with jupyter lab. Please use jupyter notebook instead.\n",
    "\n",
    "        theta = fitted_model.theta\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca(projection='3d')\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "        c = np.zeros_like(X[:,1], dtype=float)\n",
    "        c[y == 1] = 1\n",
    "\n",
    "        ax.scatter(X[:,0], X[:,1], X[:,2], c=c, cmap=\"brg\")\n",
    "\n",
    "        x = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 100)\n",
    "        y = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 100)\n",
    "\n",
    "        x, y = np.meshgrid(x,y)\n",
    "        boundary = -(theta[1] * x + theta[2] * y + theta[0]) / theta[3]\n",
    "\n",
    "        ax.plot_surface(x, y, boundary)\n",
    "\n",
    "        ax.set_title(\"3D plot\")\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_zlabel('Feature 3')\n",
    "\n",
    "        plt.show()\n",
    "            \n",
    "    else: \n",
    "        warnings.warn('Wrong number of features. Please use dataset with 1,2 or 3 features instead.', Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_multiclass(X:np.ndarray, y:np.ndarray, n_classes:int):\n",
    "    \"\"\"\n",
    "    Decision boundary for 2 featuers for multi-class target value.\n",
    "    \"\"\"\n",
    "    \n",
    "    left_right = [np.min(X[:,0]), np.max(X[:,0])]\n",
    "    down_up = [np.min(X[:,1]), np.max(X[:,1])]\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(left_right[0], left_right[1], 500).reshape(-1, 1),\n",
    "        np.linspace(down_up[0], down_up[1], 500).reshape(-1, 1),\n",
    "    )\n",
    "    \n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "    \n",
    "    y_proba = softmax_reg.predict_proba(X_new)\n",
    "    y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "    zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        label = 'Class ' + str(i)\n",
    "        plt.plot(X[y==i, 0], X[y==i, 1], \".\", label=label)\n",
    "\n",
    "    plt.contourf(x0, x1, zz, cmap='Pastel1')\n",
    "    contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "    plt.clabel(contour, inline=1, fontsize=12)\n",
    "    plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "    plt.ylabel(\"Feature 2\", fontsize=14)\n",
    "    plt.legend(loc=\"center left\", fontsize=14)\n",
    "    plt.axis([left_right[0], left_right[1], down_up[0], down_up[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gradient Descent algorithm also for logistic regression. Ridge, Lasso and Elastic Net regularizations also works here, like for other linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(t):\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_iterations:int, theta:np.ndarray=np.array([0,0]), eta:float=0.01,\n",
    "            eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            if regularization == None:\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + alpha * np.array([0, *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = 1/self.m * self.X_b.T.dot(logistic_function(self.X_b.dot(self.theta)) - y) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path.append(self.theta)\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([1 - self.y_pred_proba, self.y_pred_proba]).T\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Stochastic Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray,  n_epochs:int, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1,\n",
    "            mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = y[self.random_sample]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([1 - self.y_pred_proba, self.y_pred_proba]).T\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Mini-batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_epochs:int, batch_size_ratio:float=0.1, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "        \n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + alpha * np.array([0, *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = 2 / self.batch_size * self.x_i.T.dot(logistic_function(self.x_i.dot(self.theta)) - self.y_i) + mix_ratio * alpha * np.array([0, *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([0, *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        return np.array([1 - self.y_pred_proba, self.y_pred_proba]).T\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.y_pred_proba = np.array(logistic_function(self.X_test_b.dot(self.theta)))\n",
    "        self.y_pred = np.zeros([len(self.y_pred_proba)])\n",
    "        self.y_pred[self.y_pred_proba >= 0.5] = 1\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Softmax Regression </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_score(X:np.ndarray, theta:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates softmax score.\n",
    "    \n",
    "    X = [[1, x1, x2, ...]\n",
    "         [1, x1, x2, ...]\n",
    "         [1, x1, x2, ...]\n",
    "         [1, x1, x2, ...]\n",
    "         [     ...      ]]\n",
    "        \n",
    "    theta = [[theta1, theta1', theta1'', ...]\n",
    "             [theta2, theta2', theta2'', ...]\n",
    "             [theta3, theta3', theta3'', ...]\n",
    "             [           ...           ]]\n",
    "    \"\"\"\n",
    "    \n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(softmax_scores:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Softmax function. Returns estimated probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    score = np.zeros_like(softmax_scores, dtype=float)\n",
    "    \n",
    "    try: \n",
    "        denominator = np.sum(np.exp(softmax_scores), axis=1)\n",
    "\n",
    "        for i in range(softmax_scores.shape[1]):\n",
    "            score[:,i] = np.exp(softmax_scores[:,i]) / denominator\n",
    "    except:\n",
    "        denominator = np.sum(np.exp(softmax_scores), axis=0)\n",
    "\n",
    "        for i in range(len(softmax_scores)):\n",
    "            score[i] = np.exp(softmax_scores[i]) / denominator\n",
    "            \n",
    "        score = np.expand_dims(score, axis=0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_classifier_prediction(estimated_probabilities:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate softmax regression classifier prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.argmax(estimated_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(X:np.ndarray, y:np.ndarray, p:np.ndarray, n_classes:int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross entropy vector for class k.\n",
    "    p - estimated probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "\n",
    "    k = n_classes\n",
    "    y_k = np.zeros([len(y), k])\n",
    "\n",
    "    for i in range(k):\n",
    "        ind = np.where(y==i)[0]\n",
    "        y_k[ind, i] = 1\n",
    "\n",
    "    if y_k.shape[0] !=1:\n",
    "        return 1/m * X.T.dot(p - y_k)\n",
    "    else:\n",
    "        X = np.expand_dims(X, axis=0)\n",
    "        return X.T.dot(p - y_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batch Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_regression_BGD:\n",
    "    \"\"\"\n",
    "    Implementation of softmax regression using Batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.p = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_iterations:int, n_classes:int, theta:np.ndarray=np.array([0,0]), eta:float=0.01,\n",
    "            eta_reducer:float=1.0, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        - lasso path tends to bounds, when some theta's numbers changes to 0 (then slopes changes abruptly).\n",
    "        So it's good idea to set eta_reducer to gradually reduce eta in order to converge to the global minimum.\n",
    "        \"\"\"\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        self.m = self.X_b.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            self.p = softmax_function(softmax_score(self.X_b, self.theta))\n",
    "            \n",
    "            if regularization == None:\n",
    "                self.gradient = cross_entropy_gradient(self.X_b, y, self.p, n_classes)\n",
    "            elif regularization == 'ridge':\n",
    "                self.gradient = cross_entropy_gradient(self.X_b, y, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "            elif regularization == 'lasso':\n",
    "                self.gradient = cross_entropy_gradient(self.X_b, y, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])])\n",
    "            elif regularization == 'elastic':\n",
    "                self.gradient = cross_entropy_gradient(self.X_b, y, self.p, n_classes) + mix_ratio * alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "            \n",
    "            self.theta = eta_reducer * (self.theta - eta * self.gradient)\n",
    "            self.theta_path.append(self.theta)\n",
    "            \n",
    "            if debugger and not np.isfinite(self.theta).all():\n",
    "                warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                break\n",
    "\n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_function(softmax_score(self.X_test_b, self.theta))\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_regression_classifier_prediction(softmax_function(softmax_score(self.X_test_b, self.theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_regression_SGD:\n",
    "    \"\"\"\n",
    "    Implementation of softmax regression using Stochastic Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_sample = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        self.p = None\n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray,  n_epochs:int, n_classes:int, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1,\n",
    "            mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_sample = np.random.randint(0, self.m)\n",
    "                self.x_i = self.X_b[self.random_sample]\n",
    "                self.y_i = np.array([y[self.random_sample]])\n",
    "                \n",
    "                self.p = softmax_function(softmax_score(self.x_i, self.theta))\n",
    "\n",
    "                if regularization == None:\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + mix_ratio * alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "                    \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_function(softmax_score(self.X_test_b, self.theta))\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_regression_classifier_prediction(softmax_function(softmax_score(self.X_test_b, self.theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mini-batch Gradient Descent </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_regression_MbGD:\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression using Mini-batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_b = None\n",
    "        self.X_test_b = None\n",
    "        self.m = None\n",
    "        self.theta_path = []\n",
    "        self.gradient = None\n",
    "        self.theta = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_proba = None\n",
    "        self.random_samples = None\n",
    "        self.x_i = None\n",
    "        self.y_i = None\n",
    "        self.p = None\n",
    "        \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, n_epochs:int, n_classes:int, batch_size_ratio:float=0.1, theta:np.ndarray=np.array([0,0]),\n",
    "            t0:float=10, t1:float=100, debugger:bool=True, alpha:float=0.1, mix_ratio:float=0.5, regularization:str=None):\n",
    "        \"\"\"\n",
    "        regularization = [None, 'ridge', 'lasso', 'elastic']\n",
    "        - ridge, lasso, elastic requires to set alpha.\n",
    "        - elastic requires to set mix_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        def learning_rate(t):\n",
    "            return t0 / (t + t1)\n",
    "\n",
    "        self.X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        if type(theta) == type(None):\n",
    "        \n",
    "            self.theta = np.array([0] * (X.shape[1] + 1))\n",
    "            self.theta_path = [self.theta]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.theta = theta\n",
    "            self.theta_path = [theta]\n",
    "        \n",
    "        self.batch_size = int(np.ceil(batch_size_ratio*len(X)))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(self.m):\n",
    "                self.random_samples = random.sample(range(0, self.m), self.batch_size)\n",
    "                self.x_i = self.X_b[self.random_samples]\n",
    "                self.y_i = y[self.random_samples]\n",
    "                \n",
    "                self.p = softmax_function(softmax_score(self.x_i, self.theta))\n",
    "                \n",
    "                if regularization == None:\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes)\n",
    "                elif regularization == 'ridge':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "                elif regularization == 'lasso':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])])\n",
    "                elif regularization == 'elastic':\n",
    "                    self.gradient = cross_entropy_gradient(self.x_i, self.y_i, self.p, n_classes) + mix_ratio * alpha * np.array([[0] * self.theta.shape[1], *np.sign(self.theta[1:])]) + (1 - mix_ratio) * alpha * np.array([[0] * self.theta.shape[1], *self.theta[1:]])\n",
    "                \n",
    "                self.eta = learning_rate(i * self.m + j)\n",
    "                self.theta = self.theta - self.eta * self.gradient\n",
    "                self.theta_path.append(self.theta)\n",
    "                \n",
    "                if debugger and not np.isfinite(self.theta).all():\n",
    "                    warnings.warn('Infinite value of theta. Further calculations may lead to errors. Path ended.', Warning)\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        self.theta_path = np.array(self.theta_path)\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_function(softmax_score(self.X_test_b, self.theta))\n",
    "    \n",
    "    def predict(self, X_test:np.ndarray) -> np.ndarray:\n",
    "        self.X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        return softmax_regression_classifier_prediction(softmax_function(softmax_score(self.X_test_b, self.theta)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
